{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "336f6d7d-9f3d-4952-85a0-02e4ab3d91e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spack/conda/miniforge3/24.3.0/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a4710d-f270-4776-989e-e7ae6d0f9176",
   "metadata": {},
   "source": [
    "## Let's compare intruction tuned (RLHF) to raw pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b62b5bc3-1cab-4ee2-973d-5cfa0973c75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "#list of models to try:     \n",
    "#model_name = 'meta-llama/Llama-3.2-1B'\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, return_dict_in_generate=True, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e62d8cd-c912-4731-8915-97b64268a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llama_response(prompt, max_new_tokens=50, temperature=0.7):\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Move to GPU if needed\n",
    "\n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=inputs[\"input_ids\"].shape[1] + max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Avoid warnings\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        # return_legacy_cache=True,  # (Optional) prevents the newer Cache object in HF>=4.47\n",
    "    )\n",
    "\n",
    "    # If outputs is a GenerationOutput (when return_dict_in_generate=True),\n",
    "    # it has a .sequences attribute. Otherwise, it's already the generated IDs.\n",
    "    if hasattr(outputs, \"sequences\"):\n",
    "        generate_ids = outputs.sequences\n",
    "    else:\n",
    "        generate_ids = outputs\n",
    "\n",
    "    # Decode\n",
    "    # generate_ids[0] is a tensor of token IDs (shape [seq_length])\n",
    "    # which is exactly what `tokenizer.decode` expects.\n",
    "    full_text = tokenizer.decode(generate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Remove the prompt to get just the completion\n",
    "    completion = full_text[len(prompt) :].strip()\n",
    "    return completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0810aa43-bd34-4200-91f8-0c89e31d4954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Explain the moon landing.\n",
      "Response: The moon landing was a historic event that took place on July 20, 1969, when NASA's Apollo 11 mission successfully landed astronauts on the surface of the moon. The mission was crewed by astronauts Neil Armstrong, Buzz Aldrin, and Michael Collins.\n",
      "The Apollo 11 spacecraft was launched from Kennedy Space Center in Florida on July 16, 1969. After traveling through space for four days, the spacecraft entered into lunar orbit. On July 20, the lunar module Eagle\n"
     ]
    }
   ],
   "source": [
    "#Example usage\n",
    "my_prompt = \"Explain the moon landing.\"\n",
    "response = generate_llama_response(my_prompt, max_new_tokens=100, temperature=0.8)\n",
    "print(\"Prompt:\", my_prompt)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8421c8ed-2e63-4328-b7f2-ce1dfe4a1a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The man felt angry, so he decided to go to the \n",
      "Response: 7-11. He bought a pack of gum, a sandwich, and a can of soda. His total bill was $15.50.\n",
      "\n",
      "## Step 1: Calculate the total cost of the items purchased.\n",
      "The total cost of the items purchased is $15.50.\n",
      "\n",
      "## Step 2: Since the man bought a pack of gum, a sandwich, and a can of soda, we need to find the cost of each item to determine the price of each.\n",
      "Let's assume the cost\n"
     ]
    }
   ],
   "source": [
    "my_prompt = \"The man felt angry, so he decided to go to the \"\n",
    "response = generate_llama_response(my_prompt, max_new_tokens=100, temperature=0.8)\n",
    "print(\"Prompt:\", my_prompt)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b645525-f984-4ca8-b71d-7f439000a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of models to try:     \n",
    "model_name = 'meta-llama/Llama-3.2-1B'\n",
    "#model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, return_dict_in_generate=True, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42bde36a-9b39-47ed-abb9-76fac336e9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Explain the moon landing.\n",
      "Response: Essay\n",
      "What is the purpose of the moon landing? What impact did it have on the human race? This paper will address these questions. The purpose of the moon landing was to see if we could land on the moon. The impact was to put a man on the moon and give man a better place on earth.\n",
      "The moon landing was a big deal to me and to many other people. I was eight years old when this happened and I remember hearing the news and learning about it. I remember\n"
     ]
    }
   ],
   "source": [
    "#Example usage\n",
    "my_prompt = \"Explain the moon landing.\"\n",
    "response = generate_llama_response(my_prompt, max_new_tokens=100, temperature=0.8)\n",
    "print(\"Prompt:\", my_prompt)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44a9a3-2663-4083-ab86-0b883e5ae202",
   "metadata": {},
   "source": [
    "## Let's load back the intruction tuned model and see what effect temperature has on the model's next token probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93ee0b7e-6706-4a13-8995-aa8141255d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "#list of models to try:     \n",
    "#model_name = 'meta-llama/Llama-3.2-1B'\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "681be07e-4781-4a20-b8d7-6f591e847d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_generate_with_top_k_sampling(\n",
    "    prompt,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_new_tokens=50,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Step-by-step generation using top-k sampling (plus temperature).\n",
    "    \n",
    "    Args:\n",
    "      prompt (str): initial text to start generating from\n",
    "      model: a causal LM from transformers (on the correct device)\n",
    "      tokenizer: corresponding tokenizer\n",
    "      max_new_tokens (int): how many new tokens to generate\n",
    "      temperature (float): temperature for sampling\n",
    "      top_k (int): how many top tokens to keep each step\n",
    "\n",
    "    Returns:\n",
    "      (final_text, generation_steps) where:\n",
    "        final_text is the decoded string (prompt + new tokens)\n",
    "        generation_steps is a list of dicts, each with:\n",
    "          - 'step': which decoding step\n",
    "          - 'chosen_token': the actual token chosen\n",
    "          - 'chosen_prob': the *raw* probability for that chosen token\n",
    "          - 'top_k_tokens': list of (token_str, raw_prob) *before* renormalizing\n",
    "    \"\"\"\n",
    "\n",
    "    # Encode the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "\n",
    "    generation_steps = []\n",
    "\n",
    "    for step in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            # logits shape: [batch_size=1, seq_len, vocab_size]\n",
    "            logits = outputs.logits[:, -1, :]  # distribution for the next token\n",
    "\n",
    "        # Apply temperature\n",
    "        if temperature != 1.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "        # Convert logits -> probabilities\n",
    "        probs = F.softmax(logits, dim=-1).squeeze(0)  # shape [vocab_size]\n",
    "\n",
    "        # Get top-k subset (or fewer if vocab < top_k)\n",
    "        if top_k <= 0 or top_k >= probs.size(0):\n",
    "            # top_k <= 0 means \"no pruning\" – sample from all tokens\n",
    "            top_k_probs = probs\n",
    "            top_k_ids = torch.arange(probs.size(0), device=probs.device)\n",
    "        else:\n",
    "            top_k_probs, top_k_ids = torch.topk(probs, top_k)\n",
    "        \n",
    "        # Store the *raw* probabilities of the top-k tokens for logging\n",
    "        top_k_tokens_info = []\n",
    "        for i in range(top_k_probs.size(0)):\n",
    "            tid = top_k_ids[i].item()\n",
    "            tk_str = tokenizer.decode([tid])\n",
    "            tk_prob = top_k_probs[i].item()\n",
    "            top_k_tokens_info.append((tk_str, tk_prob))\n",
    "\n",
    "        # Renormalize the top-k probs to sum to 1\n",
    "        top_k_probs = top_k_probs / top_k_probs.sum()\n",
    "\n",
    "        # Sample from the top-k distribution\n",
    "        chosen_idx_in_top_k = torch.multinomial(top_k_probs, 1)\n",
    "        chosen_token_id = top_k_ids[chosen_idx_in_top_k].item()\n",
    "        chosen_prob_raw = probs[chosen_token_id].item()  # from the *original* distribution\n",
    "        chosen_token_str = tokenizer.decode([chosen_token_id])\n",
    "\n",
    "        # Store step info\n",
    "        generation_steps.append({\n",
    "            \"step\": step,\n",
    "            \"chosen_token\": chosen_token_str,\n",
    "            \"chosen_prob\": chosen_prob_raw,   # raw probability in the full vocab\n",
    "            \"top_k_tokens\": top_k_tokens_info # raw probabilities for the top_k\n",
    "        })\n",
    "\n",
    "        # Append chosen token to input_ids\n",
    "        next_token_id_tensor = torch.tensor([[chosen_token_id]], device=input_ids.device)\n",
    "        input_ids = torch.cat([input_ids, next_token_id_tensor], dim=1)\n",
    "\n",
    "        # Optional: stop if EOS\n",
    "        # if chosen_token_id == tokenizer.eos_token_id:\n",
    "        #     break\n",
    "\n",
    "    # Decode the entire generated sequence\n",
    "    final_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return final_text, generation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "463b8997-c01e-49c9-94d7-312249dfcd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final text:\n",
      " The man got so angry that he decided to leave his wife for her sister, who worked at a local restaurant. The wife found out and followed\n",
      "\n",
      "Step-by-step sampling:\n",
      "Step 0 -> ' leave' (prob=0.0357)\n",
      "    Top-1: ' take' prob=0.2910\n",
      "    Top-2: ' go' prob=0.0954\n",
      "    Top-3: ' write' prob=0.0378\n",
      "    Top-4: ' leave' prob=0.0357\n",
      "    Top-5: ' do' prob=0.0196\n",
      "    Top-6: ' make' prob=0.0181\n",
      "    Top-7: ' fight' prob=0.0177\n",
      "    Top-8: ' quit' prob=0.0171\n",
      "    Top-9: ' run' prob=0.0156\n",
      "    Top-10: ' throw' prob=0.0142\n",
      "Step 1 -> ' his' (prob=0.2813)\n",
      "    Top-1: ' the' prob=0.5402\n",
      "    Top-2: ' his' prob=0.2813\n",
      "    Top-3: ' a' prob=0.0515\n",
      "    Top-4: '.' prob=0.0342\n",
      "    Top-5: ' and' prob=0.0179\n",
      "    Top-6: '.\\n' prob=0.0127\n",
      "    Top-7: ' town' prob=0.0120\n",
      "    Top-8: ',' prob=0.0092\n",
      "    Top-9: '\\n' prob=0.0075\n",
      "    Top-10: ' home' prob=0.0036\n",
      "Step 2 -> ' wife' (prob=0.3776)\n",
      "    Top-1: ' wife' prob=0.3776\n",
      "    Top-2: ' job' prob=0.1671\n",
      "    Top-3: ' house' prob=0.1425\n",
      "    Top-4: ' home' prob=0.1057\n",
      "    Top-5: ' family' prob=0.0198\n",
      "    Top-6: ' girlfriend' prob=0.0150\n",
      "    Top-7: ' car' prob=0.0145\n",
      "    Top-8: ' apartment' prob=0.0135\n",
      "    Top-9: ' marriage' prob=0.0088\n",
      "    Top-10: ' anger' prob=0.0051\n",
      "Step 3 -> ' for' (prob=0.0980)\n",
      "    Top-1: ' and' prob=0.6252\n",
      "    Top-2: '.' prob=0.1119\n",
      "    Top-3: ' for' prob=0.0980\n",
      "    Top-4: ',' prob=0.0508\n",
      "    Top-5: '.\\n' prob=0.0231\n",
      "    Top-6: \"'s\" prob=0.0175\n",
      "    Top-7: ' because' prob=0.0163\n",
      "    Top-8: ' in' prob=0.0081\n",
      "    Top-9: ' to' prob=0.0058\n",
      "    Top-10: ' after' prob=0.0047\n",
      "Step 4 -> ' her' (prob=0.1442)\n",
      "    Top-1: ' another' prob=0.5638\n",
      "    Top-2: ' her' prob=0.1442\n",
      "    Top-3: ' a' prob=0.1003\n",
      "    Top-4: ' his' prob=0.0933\n",
      "    Top-5: ' the' prob=0.0394\n",
      "    Top-6: ' someone' prob=0.0369\n",
      "    Top-7: ' one' prob=0.0040\n",
      "    Top-8: ' an' prob=0.0031\n",
      "    Top-9: '...' prob=0.0022\n",
      "    Top-10: ' that' prob=0.0017\n",
      "Step 5 -> ' sister' (prob=0.7052)\n",
      "    Top-1: ' sister' prob=0.7052\n",
      "    Top-2: '.' prob=0.0512\n",
      "    Top-3: ' best' prob=0.0462\n",
      "    Top-4: ' younger' prob=0.0240\n",
      "    Top-5: ',' prob=0.0213\n",
      "    Top-6: ' rival' prob=0.0169\n",
      "    Top-7: '.\\n' prob=0.0160\n",
      "    Top-8: ' friend' prob=0.0086\n",
      "    Top-9: ' lover' prob=0.0072\n",
      "    Top-10: ' ex' prob=0.0068\n",
      "Step 6 -> ',' (prob=0.1698)\n",
      "    Top-1: '.' prob=0.4934\n",
      "    Top-2: '.\\n' prob=0.1956\n",
      "    Top-3: ',' prob=0.1698\n",
      "    Top-4: '\\n' prob=0.0497\n",
      "    Top-5: '!' prob=0.0155\n",
      "    Top-6: '!\\n' prob=0.0147\n",
      "    Top-7: '.\\n\\n' prob=0.0091\n",
      "    Top-8: '-in' prob=0.0077\n",
      "    Top-9: ' and' prob=0.0067\n",
      "    Top-10: \"'s\" prob=0.0048\n",
      "Step 7 -> ' who' (prob=0.2395)\n",
      "    Top-1: ' who' prob=0.2395\n",
      "    Top-2: ' but' prob=0.1874\n",
      "    Top-3: ' and' prob=0.1176\n",
      "    Top-4: ' a' prob=0.0522\n",
      "    Top-5: ' whom' prob=0.0489\n",
      "    Top-6: ' which' prob=0.0461\n",
      "    Top-7: ' the' prob=0.0316\n",
      "    Top-8: ' because' prob=0.0263\n",
      "    Top-9: ' his' prob=0.0230\n",
      "    Top-10: ' with' prob=0.0101\n",
      "Step 8 -> ' worked' (prob=0.0062)\n",
      "    Top-1: ' was' prob=0.5977\n",
      "    Top-2: ' he' prob=0.1289\n",
      "    Top-3: ' had' prob=0.1162\n",
      "    Top-4: ' is' prob=0.0350\n",
      "    Top-5: ' lived' prob=0.0139\n",
      "    Top-6: ' turned' prob=0.0118\n",
      "    Top-7: ' just' prob=0.0103\n",
      "    Top-8: ' happened' prob=0.0097\n",
      "    Top-9: ' looked' prob=0.0072\n",
      "    Top-10: ' worked' prob=0.0062\n",
      "Step 9 -> ' at' (prob=0.2780)\n",
      "    Top-1: ' as' prob=0.6217\n",
      "    Top-2: ' at' prob=0.2780\n",
      "    Top-3: ' in' prob=0.0768\n",
      "    Top-4: ' for' prob=0.0107\n",
      "    Top-5: ' on' prob=0.0027\n",
      "    Top-6: ' the' prob=0.0014\n",
      "    Top-7: ' with' prob=0.0013\n",
      "    Top-8: ' out' prob=0.0012\n",
      "    Top-9: ' a' prob=0.0008\n",
      "    Top-10: ' from' prob=0.0008\n",
      "Step 10 -> ' a' (prob=0.4643)\n",
      "    Top-1: ' the' prob=0.4859\n",
      "    Top-2: ' a' prob=0.4643\n",
      "    Top-3: ' his' prob=0.0178\n",
      "    Top-4: ' an' prob=0.0077\n",
      "    Top-5: ' Starbucks' prob=0.0067\n",
      "    Top-6: ' McDonald' prob=0.0056\n",
      "    Top-7: ' Walmart' prob=0.0014\n",
      "    Top-8: ' her' prob=0.0010\n",
      "    Top-9: ' their' prob=0.0007\n",
      "    Top-10: ' one' prob=0.0005\n",
      "Step 11 -> ' local' (prob=0.4291)\n",
      "    Top-1: ' local' prob=0.4291\n",
      "    Top-2: ' nearby' prob=0.2163\n",
      "    Top-3: ' restaurant' prob=0.0456\n",
      "    Top-4: ' bar' prob=0.0295\n",
      "    Top-5: ' bakery' prob=0.0153\n",
      "    Top-6: ' store' prob=0.0096\n",
      "    Top-7: ' fast' prob=0.0074\n",
      "    Top-8: ' pizza' prob=0.0073\n",
      "    Top-9: ' coffee' prob=0.0061\n",
      "    Top-10: ' bank' prob=0.0059\n",
      "Step 12 -> ' restaurant' (prob=0.3668)\n",
      "    Top-1: ' restaurant' prob=0.3668\n",
      "    Top-2: ' bar' prob=0.1152\n",
      "    Top-3: ' bakery' prob=0.0428\n",
      "    Top-4: ' store' prob=0.0257\n",
      "    Top-5: ' convenience' prob=0.0191\n",
      "    Top-6: ' coffee' prob=0.0179\n",
      "    Top-7: ' diner' prob=0.0164\n",
      "    Top-8: ' grocery' prob=0.0133\n",
      "    Top-9: ' business' prob=0.0128\n",
      "    Top-10: ' bank' prob=0.0120\n",
      "Step 13 -> '.' (prob=0.7162)\n",
      "    Top-1: '.' prob=0.7162\n",
      "    Top-2: '.\\n' prob=0.2200\n",
      "    Top-3: ',' prob=0.0156\n",
      "    Top-4: '.\\n\\n' prob=0.0105\n",
      "    Top-5: ' in' prob=0.0086\n",
      "    Top-6: '\\n' prob=0.0059\n",
      "    Top-7: ' and' prob=0.0052\n",
      "    Top-8: ' where' prob=0.0040\n",
      "    Top-9: ' as' prob=0.0030\n",
      "    Top-10: ' that' prob=0.0018\n",
      "Step 14 -> ' The' (prob=0.2570)\n",
      "    Top-1: ' He' prob=0.3505\n",
      "    Top-2: ' The' prob=0.2570\n",
      "    Top-3: ' But' prob=0.0609\n",
      "    Top-4: ' However' prob=0.0492\n",
      "    Top-5: ' This' prob=0.0417\n",
      "    Top-6: ' His' prob=0.0242\n",
      "    Top-7: ' (' prob=0.0202\n",
      "    Top-8: ' When' prob=0.0170\n",
      "    Top-9: ' I' prob=0.0114\n",
      "    Top-10: ' She' prob=0.0103\n",
      "Step 15 -> ' wife' (prob=0.3178)\n",
      "    Top-1: ' man' prob=0.3493\n",
      "    Top-2: ' wife' prob=0.3178\n",
      "    Top-3: ' woman' prob=0.0627\n",
      "    Top-4: ' sister' prob=0.0446\n",
      "    Top-5: ' husband' prob=0.0399\n",
      "    Top-6: ' couple' prob=0.0192\n",
      "    Top-7: ' reason' prob=0.0142\n",
      "    Top-8: ' problem' prob=0.0093\n",
      "    Top-9: ' anger' prob=0.0083\n",
      "    Top-10: ' two' prob=0.0062\n",
      "Step 16 -> ' found' (prob=0.1351)\n",
      "    Top-1: ' was' prob=0.3437\n",
      "    Top-2: ',' prob=0.2353\n",
      "    Top-3: ' found' prob=0.1351\n",
      "    Top-4: ' discovered' prob=0.0497\n",
      "    Top-5: ' had' prob=0.0387\n",
      "    Top-6: ' and' prob=0.0267\n",
      "    Top-7: ' felt' prob=0.0236\n",
      "    Top-8: \"'s\" prob=0.0207\n",
      "    Top-9: ' knew' prob=0.0118\n",
      "    Top-10: ' heard' prob=0.0099\n",
      "Step 17 -> ' out' (prob=0.9713)\n",
      "    Top-1: ' out' prob=0.9713\n",
      "    Top-2: ' the' prob=0.0100\n",
      "    Top-3: ' this' prob=0.0075\n",
      "    Top-4: ' a' prob=0.0033\n",
      "    Top-5: ' her' prob=0.0023\n",
      "    Top-6: ' him' prob=0.0017\n",
      "    Top-7: ' it' prob=0.0013\n",
      "    Top-8: ' herself' prob=0.0008\n",
      "    Top-9: ' his' prob=0.0005\n",
      "    Top-10: ' that' prob=0.0003\n",
      "Step 18 -> ' and' (prob=0.5399)\n",
      "    Top-1: ' and' prob=0.5399\n",
      "    Top-2: ' about' prob=0.4121\n",
      "    Top-3: ' that' prob=0.0116\n",
      "    Top-4: ' when' prob=0.0089\n",
      "    Top-5: ' what' prob=0.0065\n",
      "    Top-6: ',' prob=0.0042\n",
      "    Top-7: ' she' prob=0.0026\n",
      "    Top-8: ' through' prob=0.0022\n",
      "    Top-9: ' after' prob=0.0018\n",
      "    Top-10: ' the' prob=0.0017\n",
      "Step 19 -> ' followed' (prob=0.0067)\n",
      "    Top-1: ' was' prob=0.3919\n",
      "    Top-2: ' confronted' prob=0.2251\n",
      "    Top-3: ' got' prob=0.0332\n",
      "    Top-4: ' became' prob=0.0329\n",
      "    Top-5: ' went' prob=0.0249\n",
      "    Top-6: ' decided' prob=0.0238\n",
      "    Top-7: ' tried' prob=0.0228\n",
      "    Top-8: ' felt' prob=0.0189\n",
      "    Top-9: ',' prob=0.0188\n",
      "    Top-10: ' threatened' prob=0.0140\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The man got so angry that he decided to\"\n",
    "final_text, steps = stepwise_generate_with_top_k_sampling(\n",
    "    prompt,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=20,\n",
    "    temperature=0.8,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Final text:\\n\", final_text)\n",
    "\n",
    "print(\"\\nStep-by-step sampling:\")\n",
    "for step_info in steps:\n",
    "    step_idx = step_info[\"step\"]\n",
    "    chosen_token = step_info[\"chosen_token\"]\n",
    "    chosen_prob = step_info[\"chosen_prob\"]\n",
    "    top_tokens = step_info[\"top_k_tokens\"]  # (token_str, raw_prob)\n",
    "    \n",
    "    # Print step + chosen token\n",
    "    print(f\"Step {step_idx} -> '{chosen_token}' (prob={chosen_prob:.4f})\")\n",
    "\n",
    "    # Print only the first 10 of the top_k tokens\n",
    "    for i, (tk_str, tk_prob) in enumerate(top_tokens[:10]):\n",
    "        print(f\"    Top-{i+1}: {repr(tk_str)} prob={tk_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72868458-f0ce-4487-acbb-23be73714545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final text:\n",
      " The man got so angry that he decided to take drastic measures. He became obsessed with finding a new hobby and started taking classes at a local community\n",
      "\n",
      "Step-by-step sampling:\n",
      "Step 0 -> ' take' (prob=0.2910)\n",
      "    Top-1: ' take' prob=0.2910\n",
      "    Top-2: ' go' prob=0.0954\n",
      "    Top-3: ' write' prob=0.0378\n",
      "    Top-4: ' leave' prob=0.0357\n",
      "    Top-5: ' do' prob=0.0196\n",
      "    Top-6: ' make' prob=0.0181\n",
      "    Top-7: ' fight' prob=0.0177\n",
      "    Top-8: ' quit' prob=0.0171\n",
      "    Top-9: ' run' prob=0.0156\n",
      "    Top-10: ' throw' prob=0.0142\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The man got so angry that he decided to\"\n",
    "final_text, steps = stepwise_generate_with_top_k_sampling(\n",
    "    prompt,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=20,\n",
    "    temperature=0.8,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Final text:\\n\", final_text)\n",
    "\n",
    "print(\"\\nStep-by-step sampling:\")\n",
    "count = 0\n",
    "for step_info in steps:\n",
    "    step_idx = step_info[\"step\"]\n",
    "    chosen_token = step_info[\"chosen_token\"]\n",
    "    chosen_prob = step_info[\"chosen_prob\"]\n",
    "    top_tokens = step_info[\"top_k_tokens\"]  # (token_str, raw_prob)\n",
    "    \n",
    "    # Print step + chosen token\n",
    "    print(f\"Step {step_idx} -> '{chosen_token}' (prob={chosen_prob:.4f})\")\n",
    "\n",
    "    # Print only the first 10 of the top_k tokens\n",
    "    for i, (tk_str, tk_prob) in enumerate(top_tokens[:10]):\n",
    "        print(f\"    Top-{i+1}: {repr(tk_str)} prob={tk_prob:.4f}\")\n",
    "    count += 1\n",
    "    if count == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8c6cad-e906-4c6d-aa96-43677f63c312",
   "metadata": {},
   "source": [
    "## Let's try a high temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8df21000-3a27-4071-864d-262e1c611bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final text:\n",
      " The man got so angry that he decided to challenge her.\n",
      "This could relate directly onto someone attacking it upon himself or some how it caused physical impact\n",
      "\n",
      "Step-by-step sampling:\n",
      "Step 0 -> ' challenge' (prob=0.0000)\n",
      "    Top-1: ' take' prob=0.0000\n",
      "    Top-2: ' go' prob=0.0000\n",
      "    Top-3: ' write' prob=0.0000\n",
      "    Top-4: ' leave' prob=0.0000\n",
      "    Top-5: ' do' prob=0.0000\n",
      "    Top-6: ' make' prob=0.0000\n",
      "    Top-7: ' fight' prob=0.0000\n",
      "    Top-8: ' quit' prob=0.0000\n",
      "    Top-9: ' run' prob=0.0000\n",
      "    Top-10: ' throw' prob=0.0000\n",
      "Step 1 -> ' her' (prob=0.0000)\n",
      "    Top-1: ' the' prob=0.0000\n",
      "    Top-2: ' his' prob=0.0000\n",
      "    Top-3: ' a' prob=0.0000\n",
      "    Top-4: ' all' prob=0.0000\n",
      "    Top-5: ' someone' prob=0.0000\n",
      "    Top-6: ' me' prob=0.0000\n",
      "    Top-7: ' anyone' prob=0.0000\n",
      "    Top-8: ' everyone' prob=0.0000\n",
      "    Top-9: ' another' prob=0.0000\n",
      "    Top-10: ' you' prob=0.0000\n",
      "Step 2 -> '.\n",
      "' (prob=0.0000)\n",
      "    Top-1: ' to' prob=0.0000\n",
      "    Top-2: '.' prob=0.0000\n",
      "    Top-3: ' in' prob=0.0000\n",
      "    Top-4: ',' prob=0.0000\n",
      "    Top-5: '.\\n' prob=0.0000\n",
      "    Top-6: ' on' prob=0.0000\n",
      "    Top-7: ' for' prob=0.0000\n",
      "    Top-8: ' and' prob=0.0000\n",
      "    Top-9: ' about' prob=0.0000\n",
      "    Top-10: ' directly' prob=0.0000\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The man got so angry that he decided to\"\n",
    "final_text, steps = stepwise_generate_with_top_k_sampling(\n",
    "    prompt,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=20,\n",
    "    temperature=10,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Final text:\\n\", final_text)\n",
    "\n",
    "print(\"\\nStep-by-step sampling:\")\n",
    "count = 0\n",
    "for step_info in steps:\n",
    "    step_idx = step_info[\"step\"]\n",
    "    chosen_token = step_info[\"chosen_token\"]\n",
    "    chosen_prob = step_info[\"chosen_prob\"]\n",
    "    top_tokens = step_info[\"top_k_tokens\"]  # (token_str, raw_prob)\n",
    "    \n",
    "    # Print step + chosen token\n",
    "    print(f\"Step {step_idx} -> '{chosen_token}' (prob={chosen_prob:.4f})\")\n",
    "\n",
    "    # Print only the first 10 of the top_k tokens\n",
    "    for i, (tk_str, tk_prob) in enumerate(top_tokens[:10]):\n",
    "        print(f\"    Top-{i+1}: {repr(tk_str)} prob={tk_prob:.4f}\")\n",
    "    count += 1\n",
    "    if count == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30be58f-352f-4da5-b8cb-2ae39f5a9bfe",
   "metadata": {},
   "source": [
    "### Let's run the eaxct same thing once more: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e7ce0a6-7850-43e4-ab19-cfc420aea836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final text:\n",
      " The man got so angry that he decided to play soccer by kicking away that stupid camera.\n",
      "Hence, the pun is used. Since the guy\n",
      "\n",
      "Step-by-step sampling:\n",
      "Step 0 -> ' play' (prob=0.0000)\n",
      "    Top-1: ' take' prob=0.0000\n",
      "    Top-2: ' go' prob=0.0000\n",
      "    Top-3: ' write' prob=0.0000\n",
      "    Top-4: ' leave' prob=0.0000\n",
      "    Top-5: ' do' prob=0.0000\n",
      "    Top-6: ' make' prob=0.0000\n",
      "    Top-7: ' fight' prob=0.0000\n",
      "    Top-8: ' quit' prob=0.0000\n",
      "    Top-9: ' run' prob=0.0000\n",
      "    Top-10: ' throw' prob=0.0000\n",
      "Step 1 -> ' soccer' (prob=0.0000)\n",
      "    Top-1: ' a' prob=0.0000\n",
      "    Top-2: ' the' prob=0.0000\n",
      "    Top-3: ' his' prob=0.0000\n",
      "    Top-4: ' with' prob=0.0000\n",
      "    Top-5: ' it' prob=0.0000\n",
      "    Top-6: ' some' prob=0.0000\n",
      "    Top-7: ' an' prob=0.0000\n",
      "    Top-8: ' music' prob=0.0000\n",
      "    Top-9: ' pr' prob=0.0000\n",
      "    Top-10: ' poker' prob=0.0000\n",
      "Step 2 -> ' by' (prob=0.0000)\n",
      "    Top-1: ' with' prob=0.0001\n",
      "    Top-2: ' in' prob=0.0000\n",
      "    Top-3: '.' prob=0.0000\n",
      "    Top-4: ' ball' prob=0.0000\n",
      "    Top-5: '.\\n' prob=0.0000\n",
      "    Top-6: ' on' prob=0.0000\n",
      "    Top-7: ',' prob=0.0000\n",
      "    Top-8: ' against' prob=0.0000\n",
      "    Top-9: ' to' prob=0.0000\n",
      "    Top-10: '\\n' prob=0.0000\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The man got so angry that he decided to\"\n",
    "final_text, steps = stepwise_generate_with_top_k_sampling(\n",
    "    prompt,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=20,\n",
    "    temperature=10,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Final text:\\n\", final_text)\n",
    "\n",
    "print(\"\\nStep-by-step sampling:\")\n",
    "count = 0\n",
    "for step_info in steps:\n",
    "    step_idx = step_info[\"step\"]\n",
    "    chosen_token = step_info[\"chosen_token\"]\n",
    "    chosen_prob = step_info[\"chosen_prob\"]\n",
    "    top_tokens = step_info[\"top_k_tokens\"]  # (token_str, raw_prob)\n",
    "    \n",
    "    # Print step + chosen token\n",
    "    print(f\"Step {step_idx} -> '{chosen_token}' (prob={chosen_prob:.4f})\")\n",
    "\n",
    "    # Print only the first 10 of the top_k tokens\n",
    "    for i, (tk_str, tk_prob) in enumerate(top_tokens[:10]):\n",
    "        print(f\"    Top-{i+1}: {repr(tk_str)} prob={tk_prob:.4f}\")\n",
    "    count += 1\n",
    "    if count == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a510bee0-88b2-4c9d-890e-8727b035995b",
   "metadata": {},
   "source": [
    "## Now let's try a much lower temperature: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1796886d-5b98-4256-aa47-ba0dd51ff0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final text:\n",
      " The man got so angry that he decided to take matters into his own hands. He stormed into the office of the local government official, demanding to\n",
      "\n",
      "Step-by-step sampling:\n",
      "Step 0 -> ' take' (prob=0.9999)\n",
      "    Top-1: ' take' prob=0.9999\n",
      "    Top-2: ' go' prob=0.0001\n",
      "    Top-3: ' write' prob=0.0000\n",
      "    Top-4: ' leave' prob=0.0000\n",
      "    Top-5: ' do' prob=0.0000\n",
      "    Top-6: ' make' prob=0.0000\n",
      "    Top-7: ' fight' prob=0.0000\n",
      "    Top-8: ' quit' prob=0.0000\n",
      "    Top-9: ' run' prob=0.0000\n",
      "    Top-10: ' throw' prob=0.0000\n",
      "Step 1 -> ' matters' (prob=0.9593)\n",
      "    Top-1: ' matters' prob=0.9593\n",
      "    Top-2: ' a' prob=0.0402\n",
      "    Top-3: ' his' prob=0.0004\n",
      "    Top-4: ' revenge' prob=0.0000\n",
      "    Top-5: ' drastic' prob=0.0000\n",
      "    Top-6: ' the' prob=0.0000\n",
      "    Top-7: ' action' prob=0.0000\n",
      "    Top-8: ' out' prob=0.0000\n",
      "    Top-9: ' up' prob=0.0000\n",
      "    Top-10: ' it' prob=0.0000\n",
      "Step 2 -> ' into' (prob=1.0000)\n",
      "    Top-1: ' into' prob=1.0000\n",
      "    Top-2: ' in' prob=0.0000\n",
      "    Top-3: ' of' prob=0.0000\n",
      "    Top-4: ' to' prob=0.0000\n",
      "    Top-5: '...' prob=0.0000\n",
      "    Top-6: '.' prob=0.0000\n",
      "    Top-7: ' himself' prob=0.0000\n",
      "    Top-8: '\\n' prob=0.0000\n",
      "    Top-9: ' out' prob=0.0000\n",
      "    Top-10: '.\\n' prob=0.0000\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The man got so angry that he decided to\"\n",
    "final_text, steps = stepwise_generate_with_top_k_sampling(\n",
    "    prompt,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=20,\n",
    "    temperature=0.1,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Final text:\\n\", final_text)\n",
    "\n",
    "print(\"\\nStep-by-step sampling:\")\n",
    "count = 0\n",
    "for step_info in steps:\n",
    "    step_idx = step_info[\"step\"]\n",
    "    chosen_token = step_info[\"chosen_token\"]\n",
    "    chosen_prob = step_info[\"chosen_prob\"]\n",
    "    top_tokens = step_info[\"top_k_tokens\"]  # (token_str, raw_prob)\n",
    "    \n",
    "    # Print step + chosen token\n",
    "    print(f\"Step {step_idx} -> '{chosen_token}' (prob={chosen_prob:.4f})\")\n",
    "\n",
    "    # Print only the first 10 of the top_k tokens\n",
    "    for i, (tk_str, tk_prob) in enumerate(top_tokens[:10]):\n",
    "        print(f\"    Top-{i+1}: {repr(tk_str)} prob={tk_prob:.4f}\")\n",
    "    count += 1\n",
    "    if count == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e2d0f2-9cdc-45c1-be5e-fd88dd053470",
   "metadata": {},
   "source": [
    "### Let's run the eaxct same thing once more: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1431a38-c0e9-4208-a9ea-b37bc4cb2f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final text:\n",
      " The man got so angry that he decided to take matters into his own hands. He stormed into the office of the local government official, demanding to\n",
      "\n",
      "Step-by-step sampling:\n",
      "Step 0 -> ' take' (prob=0.9999)\n",
      "    Top-1: ' take' prob=0.9999\n",
      "    Top-2: ' go' prob=0.0001\n",
      "    Top-3: ' write' prob=0.0000\n",
      "    Top-4: ' leave' prob=0.0000\n",
      "    Top-5: ' do' prob=0.0000\n",
      "    Top-6: ' make' prob=0.0000\n",
      "    Top-7: ' fight' prob=0.0000\n",
      "    Top-8: ' quit' prob=0.0000\n",
      "    Top-9: ' run' prob=0.0000\n",
      "    Top-10: ' throw' prob=0.0000\n",
      "Step 1 -> ' matters' (prob=0.9593)\n",
      "    Top-1: ' matters' prob=0.9593\n",
      "    Top-2: ' a' prob=0.0402\n",
      "    Top-3: ' his' prob=0.0004\n",
      "    Top-4: ' revenge' prob=0.0000\n",
      "    Top-5: ' drastic' prob=0.0000\n",
      "    Top-6: ' the' prob=0.0000\n",
      "    Top-7: ' action' prob=0.0000\n",
      "    Top-8: ' out' prob=0.0000\n",
      "    Top-9: ' up' prob=0.0000\n",
      "    Top-10: ' it' prob=0.0000\n",
      "Step 2 -> ' into' (prob=1.0000)\n",
      "    Top-1: ' into' prob=1.0000\n",
      "    Top-2: ' in' prob=0.0000\n",
      "    Top-3: ' of' prob=0.0000\n",
      "    Top-4: ' to' prob=0.0000\n",
      "    Top-5: '...' prob=0.0000\n",
      "    Top-6: '.' prob=0.0000\n",
      "    Top-7: ' himself' prob=0.0000\n",
      "    Top-8: '\\n' prob=0.0000\n",
      "    Top-9: ' out' prob=0.0000\n",
      "    Top-10: '.\\n' prob=0.0000\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The man got so angry that he decided to\"\n",
    "final_text, steps = stepwise_generate_with_top_k_sampling(\n",
    "    prompt,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=20,\n",
    "    temperature=0.1,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Final text:\\n\", final_text)\n",
    "\n",
    "print(\"\\nStep-by-step sampling:\")\n",
    "count = 0\n",
    "for step_info in steps:\n",
    "    step_idx = step_info[\"step\"]\n",
    "    chosen_token = step_info[\"chosen_token\"]\n",
    "    chosen_prob = step_info[\"chosen_prob\"]\n",
    "    top_tokens = step_info[\"top_k_tokens\"]  # (token_str, raw_prob)\n",
    "    \n",
    "    # Print step + chosen token\n",
    "    print(f\"Step {step_idx} -> '{chosen_token}' (prob={chosen_prob:.4f})\")\n",
    "\n",
    "    # Print only the first 10 of the top_k tokens\n",
    "    for i, (tk_str, tk_prob) in enumerate(top_tokens[:10]):\n",
    "        print(f\"    Top-{i+1}: {repr(tk_str)} prob={tk_prob:.4f}\")\n",
    "    count += 1\n",
    "    if count == 3:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (miniforge)",
   "language": "python",
   "name": "miniforge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
