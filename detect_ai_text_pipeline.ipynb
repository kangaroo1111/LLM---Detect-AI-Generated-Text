{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 61542,
     "databundleVersionId": 7516023,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30587,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-10-05T20:53:33.730496Z",
     "iopub.execute_input": "2024-10-05T20:53:33.730880Z",
     "iopub.status.idle": "2024-10-05T20:53:34.191067Z",
     "shell.execute_reply.started": "2024-10-05T20:53:33.730850Z",
     "shell.execute_reply": "2024-10-05T20:53:34.189879Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-19T03:42:21.162100Z",
     "start_time": "2024-10-19T03:42:20.417031Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "#load in all the data sets\n",
    "\n",
    "kaggle_train = pd.read_csv('datasets/kaggle-data/train_essays.csv') \n",
    "kaggle_prompts = pd.read_csv('datasets/kaggle-data/train_prompts.csv')\n",
    "daigt_external_data = pd.read_csv('datasets/external-data/DAIGT_concatenated.csv')\n",
    "\n",
    "# test_data_fixed = pd.read_csv('datasets/external-data/test_preprocessed_fixed.csv')\n",
    "# # train_essays_v1 = pd.read_csv('datasets/external-data/train_essays_RDizzl3_seven_v1.csv')\n",
    "# # train_data_fixed = pd.read_csv('datasets/external-data/train_preprocessed_fixed.csv')\n",
    "# # train_v2_raw = pd.read_csv('datasets/external-data/train_v2_drcat_02_raw.csv')\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:21:25.635032Z",
     "iopub.execute_input": "2024-10-03T00:21:25.635475Z",
     "iopub.status.idle": "2024-10-03T00:21:25.694723Z",
     "shell.execute_reply.started": "2024-10-03T00:21:25.635432Z",
     "shell.execute_reply": "2024-10-03T00:21:25.693515Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-19T04:07:50.088239Z",
     "start_time": "2024-10-19T04:07:47.978523Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "# Count 1s and 0s in 'generated' column for kaggle_train\n",
    "kaggle_train_counts = kaggle_train['generated'].value_counts()\n",
    "\n",
    "# Count 1s and 0s in 'generated' column for daigt_external_data\n",
    "daigt_external_counts = daigt_external_data['generated'].value_counts()\n",
    "\n",
    "# Display results\n",
    "print(\"Kaggle Train - 'generated' column counts:\")\n",
    "print(kaggle_train_counts)\n",
    "\n",
    "print(\"\\nDAIGT External Data - 'generated' column counts:\")\n",
    "print(daigt_external_counts)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:21:25.706284Z",
     "iopub.execute_input": "2024-10-03T00:21:25.706584Z",
     "iopub.status.idle": "2024-10-03T00:21:25.720798Z",
     "shell.execute_reply.started": "2024-10-03T00:21:25.706558Z",
     "shell.execute_reply": "2024-10-03T00:21:25.719689Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-19T04:13:19.009417Z",
     "start_time": "2024-10-19T04:13:18.999581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle Train - 'generated' column counts:\n",
      "generated\n",
      "0    1375\n",
      "1       3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "DAIGT External Data - 'generated' column counts:\n",
      "generated\n",
      "0    29907\n",
      "1    24784\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Based on the above output (Kaggle Train - 'generated' column counts:\n",
    "0    1375\n",
    "1       3\n",
    "DAIGT External Data - 'generated' column counts:\n",
    "0    29907\n",
    "1    24784), \n",
    "we disregard the Kaggle training set and work with the compiled data externel DAIGT data set instead."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:15:59.179805Z",
     "start_time": "2024-10-19T04:15:59.166286Z"
    }
   },
   "cell_type": "code",
   "source": "daigt_external_data.head()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         id prompt_id                                               text  \\\n",
       "0  d429f032         0  Advantages of Limiting Car Usage \\n\\nLimiting ...   \n",
       "1  1ce279be         0  Advantages of Limiting Car Usage\\n\\nLimiting c...   \n",
       "2  c9595213         0  Limiting car usage has numerous advantages tha...   \n",
       "3  f2266d87         0  The passages provided discuss the advantages o...   \n",
       "4  eeace4bd         0  Title: The Advantages of Limiting Car Usage\\n\\...   \n",
       "\n",
       "   generated          model  kaggle_repo  \n",
       "0          1  gpt-3.5-turbo            1  \n",
       "1          1  gpt-3.5-turbo            1  \n",
       "2          1  gpt-3.5-turbo            1  \n",
       "3          1  gpt-3.5-turbo            1  \n",
       "4          1  gpt-3.5-turbo            1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "      <th>model</th>\n",
       "      <th>kaggle_repo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d429f032</td>\n",
       "      <td>0</td>\n",
       "      <td>Advantages of Limiting Car Usage \\n\\nLimiting ...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ce279be</td>\n",
       "      <td>0</td>\n",
       "      <td>Advantages of Limiting Car Usage\\n\\nLimiting c...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c9595213</td>\n",
       "      <td>0</td>\n",
       "      <td>Limiting car usage has numerous advantages tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f2266d87</td>\n",
       "      <td>0</td>\n",
       "      <td>The passages provided discuss the advantages o...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eeace4bd</td>\n",
       "      <td>0</td>\n",
       "      <td>Title: The Advantages of Limiting Car Usage\\n\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": "# Output next word probability vector using AutoModel",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "Sources: [https://huggingface.co/transformers/v3.0.2/model_doc/auto.html](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html)\n[https://stackoverflow.com/questions/76397904/generate-the-probabilities-of-all-the-next-possible-word-for-a-given-text](https://stackoverflow.com/questions/76397904/generate-the-probabilities-of-all-the-next-possible-word-for-a-given-text)\n[https://www.kaggle.com/code/funtowiczmo/hugging-face-transformers-get-started](https://www.kaggle.com/code/funtowiczmo/hugging-face-transformers-get-started)",
   "metadata": {}
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:34:01.828018Z",
     "start_time": "2024-10-19T04:33:45.295504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Desktop Folders\\Coding\\LLM---Detect-AI-Generated-Text\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b43cd39850>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:34:07.135085Z",
     "start_time": "2024-10-19T04:34:07.072872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split into training and testing sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(\n",
    "    daigt_external_data, test_size=0.2, random_state=42, stratify=daigt_external_data['generated']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Testing samples: {len(test_data)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 43752\n",
      "Testing samples: 10939\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:37:31.838870Z",
     "start_time": "2024-10-19T04:37:31.825351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM , AutoTokenizer\n",
    "\n",
    "class LMHeadModel:\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        # Initialize the model and the tokenizer.\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    def get_predictions(self, sentence):\n",
    "        # Encode the sentence using the tokenizer and return the model predictions.\n",
    "        inputs = self.tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(inputs)\n",
    "            predictions = outputs[0]\n",
    "        return predictions\n",
    "    \n",
    "    def get_next_word_probabilities(self, sentence, top_k=500):\n",
    "\n",
    "        # Get the model predictions for the sentence.\n",
    "        predictions = self.get_predictions(sentence)\n",
    "        \n",
    "        # Get the next token candidates.\n",
    "        next_token_candidates_tensor = predictions[0, -1, :]\n",
    "\n",
    "        # Get the top k next token candidates.\n",
    "        topk_candidates_indexes = torch.topk(\n",
    "            next_token_candidates_tensor, top_k).indices.tolist()\n",
    "\n",
    "        # Get the token probabilities for all candidates.\n",
    "        all_candidates_probabilities = torch.nn.functional.softmax(\n",
    "            next_token_candidates_tensor, dim=-1)\n",
    "        \n",
    "        # Filter the token probabilities for the top k candidates.\n",
    "        topk_candidates_probabilities = \\\n",
    "            all_candidates_probabilities[topk_candidates_indexes].tolist()\n",
    "\n",
    "        # Decode the top k candidates back to words.\n",
    "        topk_candidates_tokens = \\\n",
    "            [self.tokenizer.decode([idx]).strip() for idx in topk_candidates_indexes]\n",
    "\n",
    "        # Return the top k candidates and their probabilities.\n",
    "        return list(zip(topk_candidates_tokens, topk_candidates_probabilities))"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T05:12:22.728076Z",
     "start_time": "2024-10-19T05:11:35.755837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the language model (e.g., GPT-2)\n",
    "#list of models to try:     \n",
    "    # model_name = 'gpt2'\n",
    "    # model_name = 'meta-llama/Llama-3.2-1B', \n",
    "    # model_name = 'mistralai/Mistral-7B-v0.1' \n",
    "    # model_name = 'EleutherAI/gpt-neo-125M'\n",
    "    # model_name = 'distilgpt2'\n",
    "    # model_name = 'tiiuae/falcon-7b'\n",
    "model_name = 'EleutherAI/gpt-neo-125M'  # You can replace this with 'gpt2-medium', 'gpt2-large', etc.\n",
    "lm_model = LMHeadModel(model_name)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Desktop Folders\\Coding\\LLM---Detect-AI-Generated-Text\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\farha\\.cache\\huggingface\\hub\\models--EleutherAI--gpt-neo-125M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T05:15:40.871950Z",
     "start_time": "2024-10-19T05:15:40.860156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_text(text, lm_model, num_splits=50):\n",
    "    \"\"\"\n",
    "    Process a single text to generate a vector of true next word probabilities.\n",
    "    \"\"\"\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    probabilities = []\n",
    "\n",
    "    # Ensure there are enough words to sample\n",
    "    if len(words) < 2:\n",
    "        return [0.0] * num_splits  # Return zeros if not enough words\n",
    "\n",
    "    # Generate 50 split points\n",
    "    for _ in range(num_splits):\n",
    "        # Randomly select a word index (excluding the last word)\n",
    "        split_idx = np.random.randint(1, len(words))\n",
    "        context_words = words[:split_idx]\n",
    "        true_next_word = words[split_idx]\n",
    "\n",
    "        # Reconstruct the context sentence\n",
    "        context_sentence = ' '.join(context_words)\n",
    "\n",
    "        # Get the next word probabilities\n",
    "        try:\n",
    "            next_word_probs = lm_model.get_next_word_probabilities(context_sentence, top_k=500)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing context: {e}\")\n",
    "            probabilities.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Find the probability of the true next word\n",
    "        true_word_prob = 0.0\n",
    "        for word, prob in next_word_probs:\n",
    "            if word == true_next_word:\n",
    "                true_word_prob = prob\n",
    "                break  # Stop searching once found\n",
    "\n",
    "        probabilities.append(true_word_prob)\n",
    "\n",
    "    return probabilities\n"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T05:16:30.943607Z",
     "start_time": "2024-10-19T05:15:44.940458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare lists to store the results\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Process training data\n",
    "print(\"Processing training data...\")\n",
    "for idx, row in tqdm(train_data.iterrows(), total=len(train_data)):\n",
    "    text = row['text']\n",
    "    label = row['generated']\n",
    "    probs_vector = process_text(text, lm_model)\n",
    "    X_train.append(probs_vector)\n",
    "    y_train.append(label)\n",
    "\n",
    "# Convert lists to tensors or arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "# Save the numpy arrays to .npy files\n",
    "np.save('X_train.npy', X_train)\n",
    "np.save('y_train.npy', y_train)\n",
    "\n",
    "# Similarly process the test data\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "print(\"Processing testing data...\")\n",
    "for idx, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "    text = row['text']\n",
    "    label = row['generated']\n",
    "    probs_vector = process_text(text, lm_model)\n",
    "    X_test.append(probs_vector)\n",
    "    y_test.append(label)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "# Save the numpy arrays to .npy files\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('y_test.npy', y_test)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/43752 [00:45<550:37:25, 45.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m text \u001B[38;5;241m=\u001B[39m row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      9\u001B[0m label \u001B[38;5;241m=\u001B[39m row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgenerated\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m---> 10\u001B[0m probs_vector \u001B[38;5;241m=\u001B[39m \u001B[43mprocess_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m X_train\u001B[38;5;241m.\u001B[39mappend(probs_vector)\n\u001B[0;32m     12\u001B[0m y_train\u001B[38;5;241m.\u001B[39mappend(label)\n",
      "Cell \u001B[1;32mIn[37], line 25\u001B[0m, in \u001B[0;36mprocess_text\u001B[1;34m(text, lm_model, num_splits)\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# Get the next word probabilities\u001B[39;00m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 25\u001B[0m     next_word_probs \u001B[38;5;241m=\u001B[39m \u001B[43mlm_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_next_word_probabilities\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext_sentence\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError processing context: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[28], line 22\u001B[0m, in \u001B[0;36mLMHeadModel.get_next_word_probabilities\u001B[1;34m(self, sentence, top_k)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_next_word_probabilities\u001B[39m(\u001B[38;5;28mself\u001B[39m, sentence, top_k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m):\n\u001B[0;32m     20\u001B[0m \n\u001B[0;32m     21\u001B[0m     \u001B[38;5;66;03m# Get the model predictions for the sentence.\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_predictions\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentence\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;66;03m# Get the next token candidates.\u001B[39;00m\n\u001B[0;32m     25\u001B[0m     next_token_candidates_tensor \u001B[38;5;241m=\u001B[39m predictions[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :]\n",
      "Cell \u001B[1;32mIn[28], line 15\u001B[0m, in \u001B[0;36mLMHeadModel.get_predictions\u001B[1;34m(self, sentence)\u001B[0m\n\u001B[0;32m     13\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mencode(sentence, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 15\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m predictions\n",
      "File \u001B[1;32mC:\\Desktop Folders\\Coding\\LLM---Detect-AI-Generated-Text\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Desktop Folders\\Coding\\LLM---Detect-AI-Generated-Text\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mC:\\Desktop Folders\\Coding\\LLM---Detect-AI-Generated-Text\\venv\\lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:1043\u001B[0m, in \u001B[0;36mGPTNeoForCausalLM.forward\u001B[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[0m\n\u001B[0;32m   1035\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1036\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[0;32m   1037\u001B[0m \u001B[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001B[39;00m\n\u001B[0;32m   1038\u001B[0m \u001B[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001B[39;00m\n\u001B[0;32m   1039\u001B[0m \u001B[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001B[39;00m\n\u001B[0;32m   1040\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1041\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m-> 1043\u001B[0m transformer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1044\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1045\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1046\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1047\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1048\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1049\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1050\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1051\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1052\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1053\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1054\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1055\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1056\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1057\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m transformer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1059\u001B[0m lm_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head(hidden_states)\n",
      "File \u001B[1;32mC:\\Desktop Folders\\Coding\\LLM---Detect-AI-Generated-Text\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Desktop Folders\\Coding\\LLM---Detect-AI-Generated-Text\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mC:\\Desktop Folders\\Coding\\LLM---Detect-AI-Generated-Text\\venv\\lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:806\u001B[0m, in \u001B[0;36mGPTNeoModel.forward\u001B[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[0m\n\u001B[0;32m    795\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[0;32m    796\u001B[0m         block\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[0;32m    797\u001B[0m         hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    803\u001B[0m         cache_position,\n\u001B[0;32m    804\u001B[0m     )\n\u001B[0;32m    805\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 806\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    807\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    808\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_past\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    809\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    810\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    811\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    812\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    813\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    814\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    816\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    817\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[1;32mC:\\Desktop Folders\\Coding\\LLM---Detect-AI-Generated-Text\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Desktop Folders\\Coding\\LLM---Detect-AI-Generated-Text\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mC:\\Desktop Folders\\Coding\\LLM---Detect-AI-Generated-Text\\venv\\lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:529\u001B[0m, in \u001B[0;36mGPTNeoBlock.forward\u001B[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions, cache_position)\u001B[0m\n\u001B[0;32m    527\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[0;32m    528\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_2(hidden_states)\n\u001B[1;32m--> 529\u001B[0m feed_forward_hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    530\u001B[0m \u001B[38;5;66;03m# residual connection\u001B[39;00m\n\u001B[0;32m    531\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m feed_forward_hidden_states\n",
      "File \u001B[1;32mC:\\Desktop Folders\\Coding\\LLM---Detect-AI-Generated-Text\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Desktop Folders\\Coding\\LLM---Detect-AI-Generated-Text\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mC:\\Desktop Folders\\Coding\\LLM---Detect-AI-Generated-Text\\venv\\lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:486\u001B[0m, in \u001B[0;36mGPTNeoMLP.forward\u001B[1;34m(self, hidden_states)\u001B[0m\n\u001B[0;32m    484\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mc_fc(hidden_states)\n\u001B[0;32m    485\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact(hidden_states)\n\u001B[1;32m--> 486\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mc_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    487\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(hidden_states)\n\u001B[0;32m    488\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[1;32mC:\\Desktop Folders\\Coding\\LLM---Detect-AI-Generated-Text\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Desktop Folders\\Coding\\LLM---Detect-AI-Generated-Text\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mC:\\Desktop Folders\\Coding\\LLM---Detect-AI-Generated-Text\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Prepare to feed into neural net\n",
    "#Replace zeros with a small value to avoid issues in log transformation\n",
    "epsilon = 1e-10\n",
    "X_train = np.where(X_train == 0, epsilon, X_train)\n",
    "X_test = np.where(X_test == 0, epsilon, X_test)\n",
    "\n",
    "# Optionally, apply log transformation\n",
    "X_train = np.log(X_train)\n",
    "X_test = np.log(X_test)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "input_size = X_train.shape[1]\n",
    "model = FeedForwardNN(input_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Train the network\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss over the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Evaluate model\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Sanat's code is below:"
  },
  {
   "cell_type": "code",
   "source": "print(kaggle_train['text'][0])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:21:53.818537Z",
     "iopub.execute_input": "2024-10-03T00:21:53.818978Z",
     "iopub.status.idle": "2024-10-03T00:21:53.825315Z",
     "shell.execute_reply.started": "2024-10-03T00:21:53.818950Z",
     "shell.execute_reply": "2024-10-03T00:21:53.824160Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Cars. Cars have been around since they became famous in the 1900s, when Henry Ford created and built the first ModelT. Cars have played a major role in our every day lives since then. But now, people are starting to question if limiting car usage would be a good thing. To me, limiting the use of cars might be a good thing to do.\n\nIn like matter of this, article, \"In German Suburb, Life Goes On Without Cars,\" by Elizabeth Rosenthal states, how automobiles are the linchpin of suburbs, where middle class families from either Shanghai or Chicago tend to make their homes. Experts say how this is a huge impediment to current efforts to reduce greenhouse gas emissions from tailpipe. Passenger cars are responsible for 12 percent of greenhouse gas emissions in Europe...and up to 50 percent in some carintensive areas in the United States. Cars are the main reason for the greenhouse gas emissions because of a lot of people driving them around all the time getting where they need to go. Article, \"Paris bans driving due to smog,\" by Robert Duffer says, how Paris, after days of nearrecord pollution, enforced a partial driving ban to clear the air of the global city. It also says, how on Monday, motorist with evennumbered license plates were ordered to leave their cars at home or be fined a 22euro fine 31. The same order would be applied to oddnumbered plates the following day. Cars are the reason for polluting entire cities like Paris. This shows how bad cars can be because, of all the pollution that they can cause to an entire city.\n\nLikewise, in the article, \"Carfree day is spinning into a big hit in Bogota,\" by Andrew Selsky says, how programs that's set to spread to other countries, millions of Columbians hiked, biked, skated, or took the bus to work during a carfree day, leaving streets of this capital city eerily devoid of traffic jams. It was the third straight year cars have been banned with only buses and taxis permitted for the Day Without Cars in the capital city of 7 million. People like the idea of having carfree days because, it allows them to lesson the pollution that cars put out of their exhaust from people driving all the time. The article also tells how parks and sports centers have bustled throughout the city uneven, pitted sidewalks have been replaced by broad, smooth sidewalks rushhour restrictions have dramatically cut traffic and new restaurants and upscale shopping districts have cropped up. Having no cars has been good for the country of Columbia because, it has aloud them to repair things that have needed repairs for a long time, traffic jams have gone down, and restaurants and shopping districts have popped up, all due to the fact of having less cars around.\n\nIn conclusion, the use of less cars and having carfree days, have had a big impact on the environment of cities because, it is cutting down the air pollution that the cars have majorly polluted, it has aloud countries like Columbia to repair sidewalks, and cut down traffic jams. Limiting the use of cars would be a good thing for America. So we should limit the use of cars by maybe riding a bike, or maybe walking somewhere that isn't that far from you and doesn't need the use of a car to get you there. To me, limiting the use of cars might be a good thing to do.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": "sentence1, sentence2, sentence3 = kaggle_train['text'][0][:59], kaggle_train['text'][0][:350], kaggle_train['text'][0][:749]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:21:53.826617Z",
     "iopub.execute_input": "2024-10-03T00:21:53.826933Z",
     "iopub.status.idle": "2024-10-03T00:21:53.848385Z",
     "shell.execute_reply.started": "2024-10-03T00:21:53.826905Z",
     "shell.execute_reply": "2024-10-03T00:21:53.847121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": "model = LMHeadModel('bert-base-cased')\nmodel.get_next_word_probabilities(sentence1, top_k=500)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:21:53.850184Z",
     "iopub.execute_input": "2024-10-03T00:21:53.850571Z",
     "iopub.status.idle": "2024-10-03T00:22:13.779596Z",
     "shell.execute_reply.started": "2024-10-03T00:21:53.850542Z",
     "shell.execute_reply": "2024-10-03T00:22:13.778339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28cd7f407241446e9bdcbf4b214dde3c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5eaceba024394a4f888b64c5522b7dbe"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80327b96dd4e41a69aeb4001a798e4b6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7afa566a2aab46d4ba0e60eec979ec76"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9264551474e4671b1cffaccc8ded2df"
      }
     },
     "metadata": {}
    },
    {
     "execution_count": 25,
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('.', 0.9998961687088013),\n ('!', 1.0118837053596508e-05),\n (',', 9.749163837113883e-06),\n ('?', 8.029621312743984e-06),\n (';', 7.971573722898029e-06),\n ('and', 7.239583283080719e-06),\n ('\"', 6.454437880165642e-06),\n ('...', 4.2128358472837135e-06),\n ('it', 4.098200861335499e-06),\n ('It', 2.7247053822065936e-06),\n ('This', 1.9060513523072586e-06),\n ('that', 1.905687781800225e-06),\n (':', 1.5619426676494186e-06),\n ('of', 1.5250606111294474e-06),\n ('this', 1.4305770719147404e-06),\n ('the', 1.2741787713821395e-06),\n ('people', 1.200709561999247e-06),\n ('who', 9.789341675059404e-07),\n ('-', 8.085035005933605e-07),\n ('in', 7.963218422446516e-07),\n ('##s', 7.858031949581346e-07),\n ('to', 7.434784947690787e-07),\n ('Auto', 7.291108659046586e-07),\n ('have', 6.775054544050363e-07),\n ('##ly', 6.183860250530415e-07),\n ('which', 6.102171141719737e-07),\n ('what', 5.308630193212593e-07),\n ('an', 5.250657295619021e-07),\n ('a', 5.180836524232291e-07),\n ('their', 4.950094307787367e-07),\n ('He', 4.888082116849546e-07),\n (\"'\", 4.681031384734524e-07),\n ('he', 4.4954316535950056e-07),\n ('be', 4.4780378516406927e-07),\n ('/', 4.2096570496141794e-07),\n ('Cars', 4.1760287672332197e-07),\n ('That', 4.1226007851946633e-07),\n ('they', 3.852777581414557e-07),\n ('t', 3.72708882423467e-07),\n ('Museum', 3.6711048778670374e-07),\n ('The', 3.613403976032714e-07),\n ('Character', 3.610496719375078e-07),\n ('is', 3.609484338085167e-07),\n ('as', 3.383490252417687e-07),\n ('there', 3.3309925129287876e-07),\n ('them', 2.8712457833535154e-07),\n ('s', 2.835657539890235e-07),\n ('An', 2.741863340816053e-07),\n ('Their', 2.663635427779809e-07),\n ('I', 2.5838465944616473e-07),\n (')', 2.527467586332932e-07),\n ('She', 2.462140002990054e-07),\n ('They', 2.4487968630637624e-07),\n ('are', 2.1541092110055615e-07),\n ('(', 2.125304803257677e-07),\n ('she', 2.115694996973616e-07),\n ('TV', 2.067903324132203e-07),\n ('1', 1.9760619807129842e-07),\n ('for', 1.9462498812572449e-07),\n ('Art', 1.9165257469921926e-07),\n ('until', 1.8326794304357463e-07),\n ('one', 1.6270089986392122e-07),\n ('was', 1.5810516629244376e-07),\n ('People', 1.5251065121901775e-07),\n ('being', 1.472596409257676e-07),\n ('however', 1.4609943832510908e-07),\n ('when', 1.4229050293579348e-07),\n ('America', 1.4060032071938622e-07),\n ('Co', 1.3209084670506854e-07),\n ('There', 1.311495623212977e-07),\n ('from', 1.2406466964876017e-07),\n ('time', 1.2266291093965265e-07),\n ('these', 1.223009036266376e-07),\n ('er', 1.1958009338286502e-07),\n ('were', 1.1797715160355438e-07),\n ('B', 1.1785221687432568e-07),\n ('One', 1.1761137841403979e-07),\n ('because', 1.1735010474467344e-07),\n ('M', 1.164706446843411e-07),\n ('here', 1.1611363959218579e-07),\n ('International', 1.157586098088359e-07),\n ('so', 1.1009271361217543e-07),\n ('A', 1.0899496061256286e-07),\n ('am', 1.0885555212780673e-07),\n ('on', 1.0353483759217852e-07),\n ('Motor', 1.0011368800633136e-07),\n ('T', 9.77332064167058e-08),\n ('R', 9.647213516927877e-08),\n ('C', 8.359435810234572e-08),\n ('where', 8.180116850553532e-08),\n ('something', 8.007482676930522e-08),\n ('more', 7.88293235132187e-08),\n ('These', 7.446731586924216e-08),\n ('culture', 7.072377172789857e-08),\n ('too', 6.857673184867963e-08),\n ('i', 6.817244724288685e-08),\n ('Show', 6.811292507791222e-08),\n ('you', 6.723750800574635e-08),\n ('but', 6.637892369099063e-08),\n ('Z', 6.466414959049871e-08),\n ('V', 6.433494093016634e-08),\n ('your', 6.298826349393494e-08),\n ('its', 6.289954512794793e-08),\n ('her', 6.280040310002732e-08),\n ('by', 6.233725002857682e-08),\n ('Anyway', 6.136529862033058e-08),\n ('has', 6.077937797499544e-08),\n ('if', 5.999238084086755e-08),\n ('everyone', 5.849199524732285e-08),\n ('ever', 5.6564179118367974e-08),\n ('someone', 5.593238228129849e-08),\n ('his', 5.532969638011309e-08),\n ('Cinema', 5.509412304149919e-08),\n ('or', 5.485704335228547e-08),\n ('S', 5.362543831211042e-08),\n ('2', 5.119317947333002e-08),\n ('Company', 4.805083264614041e-08),\n ('Home', 4.705103506807973e-08),\n ('AD', 4.6823458887956804e-08),\n ('today', 4.590800273263085e-08),\n ('like', 4.544886067492371e-08),\n ('X', 4.4397474141533166e-08),\n ('r', 4.380054363650743e-08),\n ('Victoria', 4.361796257512651e-08),\n ('P', 4.312542500883865e-08),\n ('##eal', 4.310059154022383e-08),\n ('such', 4.250641794101284e-08),\n ('Here', 4.234941997083297e-08),\n ('His', 4.196266800704507e-08),\n ('What', 4.1872247891205916e-08),\n ('##T', 4.122048480326157e-08),\n ('Automobile', 4.098224337667489e-08),\n ('Its', 4.0528739475576003e-08),\n ('Europe', 3.930720993139403e-08),\n ('cars', 3.9091805348334674e-08),\n ('Inc', 3.893851285852179e-08),\n ('anything', 3.847620178021316e-08),\n ('had', 3.812780846601527e-08),\n ('G', 3.7917203599135973e-08),\n ('at', 3.7675338404596914e-08),\n ('E', 3.7482170256453173e-08),\n ('H', 3.720999330880659e-08),\n ('Japan', 3.5475228088444055e-08),\n ('although', 3.50810616112085e-08),\n ('##to', 3.4387639402666537e-08),\n ('Film', 3.422470840064307e-08),\n ('company', 3.418204386207435e-08),\n ('Rosa', 3.412035809446934e-08),\n ('Mexico', 3.294278272392148e-08),\n ('In', 3.291615513489887e-08),\n ('Travel', 3.2763818325065586e-08),\n ('characters', 3.1683548229466396e-08),\n ('ones', 3.155123806664051e-08),\n ('So', 3.125069980569606e-08),\n ('while', 3.091219369366627e-08),\n ('Xi', 3.061505537971243e-08),\n ('car', 3.049639829555417e-08),\n ('how', 2.998050518954187e-08),\n ('everything', 2.9679526392101252e-08),\n ('Today', 2.9584180438746444e-08),\n ('To', 2.8626429227074368e-08),\n ('women', 2.8427720621948538e-08),\n ('movement', 2.8381940353483515e-08),\n ('again', 2.813739108376012e-08),\n ('&', 2.8121188933027952e-08),\n ('Disco', 2.8097222326550764e-08),\n ('National', 2.7534859725619754e-08),\n ('age', 2.751837513415012e-08),\n ('First', 2.746698513078627e-08),\n ('Association', 2.7452477624478888e-08),\n ('Route', 2.7238426625331158e-08),\n ('industry', 2.6520352136572e-08),\n ('[UNK]', 2.642592811241684e-08),\n ('e', 2.640688379074163e-08),\n ('Oh', 2.61076493757173e-08),\n ('And', 2.5234081491021243e-08),\n ('Music', 2.500410900552197e-08),\n ('though', 2.4910667306698997e-08),\n ('Industry', 2.4890006500299933e-08),\n ('many', 2.4039385593255247e-08),\n ('c', 2.3871697507615863e-08),\n ('Digital', 2.3863048426164823e-08),\n ('France', 2.371413465596106e-08),\n ('television', 2.356349781962308e-08),\n ('y', 2.3495102752235653e-08),\n ('1930', 2.34225723261261e-08),\n ('Fun', 2.3206277788290208e-08),\n ('Am', 2.3118936098853737e-08),\n ('automobiles', 2.256728315330747e-08),\n ('every', 2.226275519490173e-08),\n ('General', 2.20638618486646e-08),\n ('order', 2.1943117545220048e-08),\n ('China', 2.1619039003439866e-08),\n ('always', 2.136863663793065e-08),\n ('Kingdom', 2.124193088093307e-08),\n ('Cartoon', 2.118932762584791e-08),\n ('##p', 2.0940227329901973e-08),\n ('##1', 2.0767046748915163e-08),\n ('##M', 2.055472059225849e-08),\n ('1910', 2.0329400385321605e-08),\n ('Everything', 2.030986756551556e-08),\n ('Her', 2.0295848557339013e-08),\n ('##li', 2.027909395962979e-08),\n ('each', 2.0161500913218333e-08),\n ('Central', 2.0140555889724965e-08),\n ('ve', 2.0019795599068857e-08),\n ('Is', 1.9928398486968035e-08),\n ('Labor', 1.9910125104161125e-08),\n ('him', 1.9897369085697392e-08),\n ('love', 1.9688451757815528e-08),\n ('DC', 1.9442007115344495e-08),\n ('1900', 1.932606430443684e-08),\n ('town', 1.927301873649867e-08),\n ('Many', 1.9143692853162975e-08),\n ('very', 1.904866309132558e-08),\n ('##sp', 1.8908323795585602e-08),\n ('since', 1.848952280170124e-08),\n ('will', 1.8341786756082e-08),\n ('Victor', 1.828687423710562e-08),\n ('0', 1.8234734611155545e-08),\n ('man', 1.8186488759397434e-08),\n ('You', 1.8155262182517617e-08),\n ('Man', 1.812598959816114e-08),\n ('h', 1.8025627213091866e-08),\n ('##H', 1.7953297515305167e-08),\n ('|', 1.758631462678295e-08),\n ('Corporation', 1.7505595195643764e-08),\n ('AC', 1.736346888492335e-08),\n ('Brand', 1.7361415416417003e-08),\n ('Foreign', 1.7211036151820736e-08),\n ('Your', 1.71331819842635e-08),\n ('Do', 1.6911528177843138e-08),\n ('know', 1.659537929299404e-08),\n ('m', 1.6416620951531513e-08),\n ('Luce', 1.6362477595066593e-08),\n ('Say', 1.6253736134785868e-08),\n ('Love', 1.6229849464366453e-08),\n ('then', 1.6083076204154167e-08),\n ('##e', 1.607059552100054e-08),\n ('Albert', 1.5930668340047305e-08),\n ('##I', 1.5884456416870307e-08),\n ('Hu', 1.585654807456649e-08),\n ('my', 1.5716025814072054e-08),\n ('##A', 1.5637684924740824e-08),\n ('etc', 1.5569115774383135e-08),\n ('##life', 1.5349387538776682e-08),\n ('Ever', 1.5244179252249523e-08),\n ('Germany', 1.5143733378408797e-08),\n ('really', 1.5094393290837615e-08),\n ('reason', 1.487864054183774e-08),\n ('Society', 1.480379641094487e-08),\n ('Hall', 1.476330524496916e-08),\n ('##ity', 1.4610216148014388e-08),\n ('Ah', 1.4517604007835416e-08),\n ('becoming', 1.4318422891790306e-08),\n ('1922', 1.429912721562232e-08),\n ('Ko', 1.4141840587456045e-08),\n ('Government', 1.4132807812927695e-08),\n ('Machine', 1.4095092204513548e-08),\n ('##ia', 1.3967941470127698e-08),\n ('Women', 1.3958913136491446e-08),\n ('Day', 1.3858264757971028e-08),\n ('those', 1.379241876264814e-08),\n ('think', 1.377393843426944e-08),\n ('last', 1.3741189519578256e-08),\n ('Television', 1.3712311286440126e-08),\n ('##C', 1.3694093858873657e-08),\n ('##L', 1.3686417332792189e-08),\n ('World', 1.3522399200383006e-08),\n ('Hollywood', 1.3504227958094361e-08),\n ('II', 1.3405298204816063e-08),\n ('around', 1.3337661641799059e-08),\n ('become', 1.3180664559797606e-08),\n ('about', 1.3001661081091243e-08),\n ('Radio', 1.298659313420103e-08),\n ('o', 1.2935124082957827e-08),\n ('##hol', 1.2895290169012696e-08),\n ('Flight', 1.2837448437608145e-08),\n ('Video', 1.2825432271768022e-08),\n ('All', 1.277028971458094e-08),\n ('##po', 1.2728099463288345e-08),\n ('Ma', 1.2566023777083046e-08),\n ('artists', 1.2561685913681231e-08),\n ('Real', 1.248089187555479e-08),\n ('Car', 1.2456538911465032e-08),\n ('anymore', 1.2391278225720725e-08),\n ('men', 1.2106451841020771e-08),\n ('Park', 1.1897748564138055e-08),\n ('##t', 1.1896500673458377e-08),\n ('anyone', 1.1881081007913963e-08),\n ('with', 1.180218767160568e-08),\n ('persons', 1.1716732473132652e-08),\n ('character', 1.1705942881690135e-08),\n ('However', 1.1680294065286034e-08),\n ('Novel', 1.1575568947819193e-08),\n ('Something', 1.1517207632039117e-08),\n ('2010', 1.1372849328949997e-08),\n ('having', 1.1357090379249257e-08),\n ('Industrial', 1.1345703043730282e-08),\n ('Main', 1.1335233196518857e-08),\n ('##ism', 1.1325789195382185e-08),\n ('machines', 1.1287337287058108e-08),\n ('David', 1.1284238432551774e-08),\n ('History', 1.1249081666164784e-08),\n ('Everyone', 1.1184363657434915e-08),\n ('Be', 1.1138933331267253e-08),\n ('all', 1.1130862453967438e-08),\n ('work', 1.1100946828435099e-08),\n ('Cha', 1.1078631345640133e-08),\n ('##co', 1.1062773808134807e-08),\n ('N', 1.104756996994638e-08),\n ('Old', 1.090217249810621e-08),\n ('himself', 1.0850537357498524e-08),\n ('University', 1.0848468789959043e-08),\n ('Ava', 1.082315126410549e-08),\n ('Local', 1.077579536712392e-08),\n ('artist', 1.0683213425011218e-08),\n ('Because', 1.0672845718318058e-08),\n ('My', 1.0635921476875865e-08),\n ('first', 1.060110665918046e-08),\n ('whenever', 1.0598276922735295e-08),\n ('family', 1.0536599148736059e-08),\n ('Mill', 1.0521277182817812e-08),\n ('revolution', 1.0501788771932752e-08),\n ('es', 1.0403541139680783e-08),\n ('Hyde', 1.0370218461730474e-08),\n ('Mom', 1.0359226365608265e-08),\n ('Death', 1.0350990287122386e-08),\n ('auto', 1.0331916655559326e-08),\n ('##y', 1.0298234265349038e-08),\n ('##X', 1.0291734575673672e-08),\n ('families', 1.0282139584205652e-08),\n ('Production', 1.0270849948312843e-08),\n ('been', 1.0252921178732777e-08),\n ('More', 1.0081534718153762e-08),\n ('became', 1.0080995593853004e-08),\n ('children', 1.00027550686832e-08),\n ('z', 9.9673318487703e-09),\n ('Rome', 9.860475991274598e-09),\n ('after', 9.841911285946026e-09),\n ('##chan', 9.811023993222534e-09),\n ('Department', 9.808049483694958e-09),\n ('Consumer', 9.775775744458315e-09),\n ('Although', 9.58706092291095e-09),\n ('J', 9.583879467811585e-09),\n ('Luna', 9.566366365731938e-09),\n ('Records', 9.5611483175162e-09),\n ('back', 9.543566825698235e-09),\n ('Business', 9.51100354029677e-09),\n ('Pen', 9.463485106664393e-09),\n ('Mo', 9.422958413551896e-09),\n ('companies', 9.407532530758544e-09),\n ('##l', 9.373358977882162e-09),\n ('quite', 9.2591561084987e-09),\n ('Life', 9.255554545006817e-09),\n ('##', 9.186137184258314e-09),\n ('into', 9.146973845020057e-09),\n ('1920', 9.126531530512239e-09),\n ('much', 9.068852335758493e-09),\n ('life', 9.043305659872658e-09),\n ('##r', 9.035202808149734e-09),\n ('em', 9.014460289336057e-09),\n ('##G', 9.003634282578332e-09),\n ('still', 8.946731355763404e-09),\n ('times', 8.93646490141009e-09),\n ('Taiwan', 8.934027739826433e-09),\n ('Canada', 8.919418981179206e-09),\n ('Of', 8.917088401005913e-09),\n ('##illa', 8.89565043848961e-09),\n ('Florence', 8.864097011951344e-09),\n ('exist', 8.849906585339795e-09),\n ('home', 8.815168150988484e-09),\n ('production', 8.814360796804976e-09),\n ('Culture', 8.812058638341114e-09),\n ('Inn', 8.788023642125609e-09),\n ('g', 8.771260162632188e-09),\n ('do', 8.762130576656091e-09),\n ('ch', 8.761679382018883e-09),\n ('But', 8.720598465572493e-09),\n ('Mansion', 8.70423022547584e-09),\n ('##uli', 8.694738262704504e-09),\n ('show', 8.677475626939213e-09),\n ('government', 8.632491166338241e-09),\n ('Mu', 8.607188739517824e-09),\n ('Such', 8.594245315407534e-09),\n ('state', 8.589689848292892e-09),\n ('City', 8.580945731750944e-09),\n ('##n', 8.534182249775313e-09),\n ('arrival', 8.456598976636087e-09),\n ('Speed', 8.455792510631e-09),\n ('##sm', 8.434546394653353e-09),\n ('school', 8.383191030247872e-09),\n ('say', 8.358124858887095e-09),\n ('Ji', 8.32517432769464e-09),\n ('said', 8.232829529219998e-09),\n ('Ch', 8.181952892982736e-09),\n ('##ci', 8.156384012636408e-09),\n ('Avalon', 8.0647124534039e-09),\n ('For', 8.02247601683348e-09),\n ('##R', 8.002379203730925e-09),\n ('Period', 7.986765027112597e-09),\n ('Cam', 7.950636593534455e-09),\n ('1929', 7.940801793893115e-09),\n ('p', 7.904368715117016e-09),\n ('ll', 7.866406193102193e-09),\n ('U', 7.852000827313077e-09),\n ('Toy', 7.84985942914318e-09),\n ('CP', 7.81354536627532e-09),\n ('some', 7.810164071031522e-09),\n ('##sca', 7.786351119420942e-09),\n ('1902', 7.729721751559282e-09),\n ('1940', 7.676783653209895e-09),\n ('Public', 7.637424026540884e-09),\n ('Men', 7.578654148687747e-09),\n ('business', 7.540817748008521e-09),\n ('me', 7.521326672588202e-09),\n ('Porsche', 7.49920836540241e-09),\n ('Every', 7.4288100115893485e-09),\n ('Go', 7.417751746174872e-09),\n ('living', 7.3848593906689075e-09),\n ('existence', 7.342667807108683e-09),\n ('manufacture', 7.3063031180709e-09),\n ('vehicles', 7.2708257192743986e-09),\n ('Very', 7.225632092655587e-09),\n ('D', 7.213679431572473e-09),\n ('music', 7.203312613057733e-09),\n ('No', 7.07412750600156e-09),\n ('AT', 7.063799323248077e-09),\n ('Angel', 7.006369262541057e-09),\n ('Animated', 6.970553023677439e-09),\n ('Age', 6.967642018906872e-09),\n ('soon', 6.963948528948549e-09),\n ('##d', 6.932142859739088e-09),\n ('3', 6.814537378829755e-09),\n ('##2', 6.798814844444223e-09),\n ('whom', 6.79375933287929e-09),\n ('1950', 6.736250224292917e-09),\n ('BC', 6.735775048838377e-09),\n ('ca', 6.721772916051805e-09),\n ('Administration', 6.690103582229767e-09),\n ('##va', 6.603440017016737e-09),\n ('May', 6.5418244155068805e-09),\n ('Dr', 6.541100550094825e-09),\n ('Then', 6.507960836898974e-09),\n ('events', 6.455566747831654e-09),\n ('than', 6.377923966738308e-09),\n ('animation', 6.349630155000341e-09),\n ('b', 6.344739400532262e-09),\n ('Max', 6.326842605375305e-09),\n ('Publishing', 6.2944760514938025e-09),\n ('Aviation', 6.279953890242496e-09),\n ('Card', 6.279354813898408e-09),\n ('de', 6.277558473044564e-09),\n ('##ch', 6.26611029730384e-09),\n ('Movement', 6.250485018455265e-09),\n ('Too', 6.207962144344492e-09),\n ('Sign', 6.205759905952846e-09),\n ('School', 6.154883269715583e-09),\n ('1928', 6.1087099822998425e-09),\n ('Earth', 6.047464307101791e-09),\n ('any', 6.0342140173474945e-09),\n ('Each', 6.003459063208538e-09),\n ('Meyer', 6.000608454570511e-09),\n ('Group', 5.994556850907884e-09),\n ('forever', 5.9924079032214195e-09),\n ('Era', 5.985611117864664e-09),\n ('along', 5.951878989662873e-09),\n ('K', 5.948553649659516e-09),\n ('usually', 5.92096771612205e-09),\n ('within', 5.9078595349149055e-09),\n ('Theater', 5.899853050550519e-09),\n ('Gracie', 5.899774002671165e-09),\n ('civilization', 5.826081839188646e-09),\n ('Santa', 5.812795134119142e-09),\n ('As', 5.79097259034711e-09),\n ('Aero', 5.76529401996595e-09),\n ('##ri', 5.755010246133452e-09),\n ('n', 5.751707554679797e-09),\n ('Alexander', 5.6687272653732634e-09),\n ('##ph', 5.630645283360991e-09),\n ('Du', 5.58325874422394e-09),\n ('museum', 5.582385664837375e-09),\n ('Paris', 5.580075512767735e-09),\n ('1945', 5.575288231085551e-09),\n ('somewhere', 5.551201276432494e-09),\n ('Jack', 5.5297491030614765e-09),\n ('Street', 5.5007260968409355e-09),\n ('want', 5.49899503710094e-09),\n ('society', 5.496855859377092e-09),\n ('Who', 5.461917584881348e-09),\n ('King', 5.452810203365743e-09),\n ('Theo', 5.445492945455044e-09),\n ('once', 5.433468341919934e-09),\n ('Turn', 5.421676885219995e-09),\n ('##ude', 5.415889070548019e-09),\n ('even', 5.4115010250654905e-09),\n ('India', 5.409642955811478e-09),\n ('York', 5.409591441463135e-09),\n ('##', 5.388809842798992e-09),\n ('go', 5.337021047324697e-09)]"
     },
     "metadata": {}
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": "class LLM_model:\n    def __init__(self, name, size, with_prompt = False):\n        self.name = name\n        self.max_input_len = size\n        self.with_prompt = with_prompt",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:22:13.780873Z",
     "iopub.execute_input": "2024-10-03T00:22:13.781489Z",
     "iopub.status.idle": "2024-10-03T00:22:13.787569Z",
     "shell.execute_reply.started": "2024-10-03T00:22:13.781458Z",
     "shell.execute_reply": "2024-10-03T00:22:13.786401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": "# Split Training Text into Segments",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#Generates a random integer partition of (a, b)\n\ndef rand_part(a, b):\n    part = []\n    x = randint(a, b)\n    part += min(x, b)\n    if part[-1]==b:\n        return part\n    else:\n        part.extend(rand_part(x, b))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:22:13.789396Z",
     "iopub.execute_input": "2024-10-03T00:22:13.790119Z",
     "iopub.status.idle": "2024-10-03T00:22:13.946533Z",
     "shell.execute_reply.started": "2024-10-03T00:22:13.790065Z",
     "shell.execute_reply": "2024-10-03T00:22:13.945285Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": "import random\ndef split_txt(begin, essay, max_seq_len = 512):\n    segments = []\n    y = min(len(essay)-1, begin + max_seq_len - 1)\n    x = random.randint(begin, y)\n    if essay[x] == ' ':\n        #segments.append(essay[:x])\n        segments.append(x)\n    else:\n        while x < y and essay[x] != ' ':\n                x+=1\n        if x == y and segments == []:\n            while essay[x-1] != ' ':\n                x-=1\n            #segments.append(essay[:x-1])\n            segments.append(x-1)\n            return segments\n        if x == y and segments != []:\n            return segments\n        else:\n            #segments.append(essay[:x])\n            segments.append(x)\n    segments.extend(split_txt(x+1, essay))\n    return segments\n        \n                ",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:22:13.948058Z",
     "iopub.execute_input": "2024-10-03T00:22:13.948537Z",
     "iopub.status.idle": "2024-10-03T00:22:13.959556Z",
     "shell.execute_reply.started": "2024-10-03T00:22:13.948496Z",
     "shell.execute_reply": "2024-10-03T00:22:13.958203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": "Example:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "split_txt(0, kaggle_train['text'][0])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:22:13.961087Z",
     "iopub.execute_input": "2024-10-03T00:22:13.961581Z",
     "iopub.status.idle": "2024-10-03T00:22:13.973543Z",
     "shell.execute_reply.started": "2024-10-03T00:22:13.961540Z",
     "shell.execute_reply": "2024-10-03T00:22:13.972414Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "execution_count": 29,
     "output_type": "execute_result",
     "data": {
      "text/plain": "[487,\n 799,\n 812,\n 972,\n 1312,\n 1485,\n 1511,\n 1800,\n 1994,\n 2234,\n 2736,\n 3137,\n 3152,\n 3244,\n 3260,\n 3282,\n 3285,\n 3285]"
     },
     "metadata": {}
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": "#what's the actual next word in the essay\n\nfrom string import punctuation\n\ndef true_next_word(essay, n):\n    word= ''\n    i = n\n    while str.isalpha(essay[i]) == False and str.isnumeric(essay[i]) == False:\n        if essay[i] in punctuation:\n            word+=essay[i]\n            return word\n        else:\n            i+=1\n    while essay[i] != ' ' and not(essay[i] in punctuation):\n        word+=essay[i]\n        i+=1\n    return word",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:22:13.975014Z",
     "iopub.execute_input": "2024-10-03T00:22:13.976018Z",
     "iopub.status.idle": "2024-10-03T00:22:13.985156Z",
     "shell.execute_reply.started": "2024-10-03T00:22:13.975971Z",
     "shell.execute_reply": "2024-10-03T00:22:13.984189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": "Example:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "true_next_word(kaggle_train['text'][0], 59)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:22:13.986562Z",
     "iopub.execute_input": "2024-10-03T00:22:13.986929Z",
     "iopub.status.idle": "2024-10-03T00:22:13.999839Z",
     "shell.execute_reply.started": "2024-10-03T00:22:13.986900Z",
     "shell.execute_reply": "2024-10-03T00:22:13.998839Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "execution_count": 31,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'1900s'"
     },
     "metadata": {}
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "source": "#homemade 'return index of element if it exists' function\n\ndef return_index(element, list):\n    i=0\n    while i < len(list):\n        if list[i] == element:\n            return i\n        else:\n            i+=1\n    if i == len(list):\n        return -10000\n    \n#get all rth elements of a list of tuples\n\ndef rths(r, list):\n    rths = []\n    for i in range(len(list)):\n        if type(list[i]) is tuple:\n            if len(list[i]) > r:\n                rths.append(list[i][r])\n            else:\n                rths.append('')\n        else:\n            rths.append('')\n    return rths\n            ",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:22:14.001131Z",
     "iopub.execute_input": "2024-10-03T00:22:14.001451Z",
     "iopub.status.idle": "2024-10-03T00:22:14.013160Z",
     "shell.execute_reply.started": "2024-10-03T00:22:14.001424Z",
     "shell.execute_reply": "2024-10-03T00:22:14.012119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": "# Prediction with prompt and source text using GPT2",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "essay_response = \"\"\"\nIn recent years, there has been a notable shift in urban planning, with an increasing focus on limiting car usage as a means to foster sustainable and environmentally friendly communities. This shift is evident in various parts of the world, as seen in the case of Vauban, Germany, where an experimental car-free community has thrived since its completion in 2006 (Rosenthal, 2009). Vauban's success challenges the conventional reliance on cars in suburban areas and serves as a model for smart planning that is gaining traction globally.\n\nVauban's innovative approach to urban development is part of a broader movement to reduce the environmental impact of cars, particularly in suburban settings where car-centric lifestyles have long been the norm. According to experts, passenger cars contribute significantly to greenhouse gas emissions, with Europe attributing 12 percent of emissions to this source, and in some car-intensive areas in the United States, the contribution climbs to a staggering 50 percent (Rosenthal, 2009). Recognizing the environmental implications, planners worldwide are reimagining suburbs, moving away from the traditional car-centric model.\n\nOne significant aspect of the shift in urban planning is the concept of \"smart planning,\" where suburbs are designed to be more compact and accessible to public transportation, reducing the need for extensive parking spaces (Rosenthal, 2009). The Vauban model encourages walking and cycling, with essential amenities placed within walking distance along main streets, challenging the conventional suburban sprawl and promoting a more sustainable lifestyle.\n\nThe movement toward limiting car usage is not limited to Europe. In Paris, the detrimental impact of car emissions on air quality led to the implementation of a partial driving ban during periods of intense smog (Duffer, 2014). The success of such initiatives is evident in the significant reduction of congestion and improvement in air quality. Similarly, Bogota, Colombia, has embraced a car-free day annually, encouraging alternative transportation methods and reducing both traffic jams and smog levels (Selsky, 2002). The success of these initiatives underscores the potential benefits of limiting car usage in diverse urban settings.\n\nIn the United States, there is a growing awareness of the need to reduce car dependency. Recent studies suggest that Americans are buying fewer cars and driving less, indicating a potential shift in cultural attitudes toward car ownership (Rosenthal, 2013). This change aligns with efforts to decrease carbon emissions, as transportation remains a major contributor to the nation's environmental footprint.\n\nWhile the trend toward limiting car usage presents challenges for the traditional automotive industry, it also opens avenues for innovation and adaptation. Companies like Ford and Mercedes are rebranding themselves as \"mobility\" companies, recognizing the evolving needs and preferences of consumers (Rosenthal, 2013). The younger generation, in particular, shows a reduced interest in car ownership, preferring alternative modes of transportation facilitated by technological advancements such as car-sharing programs and ride-sharing apps.\n\nIn conclusion, the advantages of limiting car usage extend beyond environmental benefits to encompass improved urban planning, reduced congestion, and a shift toward more sustainable lifestyles. The success stories of car-free communities in Germany, driving bans in Paris during smog episodes, and annual car-free days in Bogota demonstrate the feasibility and positive outcomes of such initiatives. As the world grapples with the environmental impact of car culture, embracing alternative transportation models becomes imperative for creating healthier, more livable communities.\n\"\"\"",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:22:14.014738Z",
     "iopub.execute_input": "2024-10-03T00:22:14.015099Z",
     "iopub.status.idle": "2024-10-03T00:22:14.034240Z",
     "shell.execute_reply.started": "2024-10-03T00:22:14.015069Z",
     "shell.execute_reply": "2024-10-03T00:22:14.032976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "source": "model1 = LLM_model('gpt2', 4096, with_prompt = True)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:22:14.035871Z",
     "iopub.execute_input": "2024-10-03T00:22:14.036289Z",
     "iopub.status.idle": "2024-10-03T00:22:14.049681Z",
     "shell.execute_reply.started": "2024-10-03T00:22:14.036255Z",
     "shell.execute_reply": "2024-10-03T00:22:14.048594Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "source": "def predict_w_prompt(text, prompt, ai):\n    split_text = split_txt(0, text, ai.max_input_len)\n    probability = 0\n    model = LMHeadModel(f'{ai.name}')\n    for splice in split_text:\n        if ai.with_prompt:\n            feed = prompt + text[:splice]\n        else:\n            feed = text[:splice]\n        p_words = model.get_next_word_probabilities(feed[max(0, len(feed)-ai.max_input_len -1):], top_k=500)\n        index = return_index(true_next_word(text, splice), rths(0, p_words))\n        print(f'index: {index}')\n        if index >= 0:\n            probability += p_words[index][1]\n            print(f'value: {p_words[index][1]}')\n        else:\n            probability += 1e-10\n        print(f'probability: {probability}')\n    output = probability/len(split_text)\n    return output",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:22:14.051258Z",
     "iopub.execute_input": "2024-10-03T00:22:14.051595Z",
     "iopub.status.idle": "2024-10-03T00:22:14.062608Z",
     "shell.execute_reply.started": "2024-10-03T00:22:14.051569Z",
     "shell.execute_reply": "2024-10-03T00:22:14.061404Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": "prompt = kaggle_prompts['source_text'][0]+kaggle_prompts['instructions'][0]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:22:14.066936Z",
     "iopub.execute_input": "2024-10-03T00:22:14.067308Z",
     "iopub.status.idle": "2024-10-03T00:22:14.076788Z",
     "shell.execute_reply.started": "2024-10-03T00:22:14.067273Z",
     "shell.execute_reply": "2024-10-03T00:22:14.075651Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": "predict_w_prompt(essay_response, prompt, model1)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:22:14.078098Z",
     "iopub.execute_input": "2024-10-03T00:22:14.078461Z",
     "iopub.status.idle": "2024-10-03T00:22:42.503275Z",
     "shell.execute_reply.started": "2024-10-03T00:22:14.078432Z",
     "shell.execute_reply": "2024-10-03T00:22:42.502151Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff754e7ba88e41df9cf1cc3e496a4485"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df59663ffa124297b69e7869d594c5dc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9f16d47c2854d718077255e03ce7a7d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ddc114d1b064aeeba404f5b2390c7e0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "583bb6a6abe24816a891b20dfe39c69e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15d2320d8eed4738856ca2686a70f36e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56bd8875686345bd874c862e1d8e522a"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "index: 1\nvalue: 0.35049864649772644\nprobability: 0.35049864649772644\nindex: 2\nvalue: 0.1668398082256317\nprobability: 0.5173384547233582\nindex: 0\nvalue: 0.9775412678718567\nprobability: 1.4948797225952148\nindex: 17\nvalue: 0.00487076910212636\nprobability: 1.4997504916973412\nindex: 78\nvalue: 0.0016030677361413836\nprobability: 1.5013535594334826\nindex: 0\nvalue: 0.9717299938201904\nprobability: 2.473083553253673\nindex: 33\nvalue: 0.0020588389597833157\nprobability: 2.4751423922134563\nindex: 0\nvalue: 0.712573230266571\nprobability: 3.1877156224800274\nindex: 1\nvalue: 0.2337258756160736\nprobability: 3.421441498096101\nindex: 1\nvalue: 0.2337258756160736\nprobability: 3.6551673737121746\n",
     "output_type": "stream"
    },
    {
     "execution_count": 37,
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.3655167373712175"
     },
     "metadata": {}
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": "model0 = LLM_model('gpt2', 4096, with_prompt = False)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:22:42.504928Z",
     "iopub.execute_input": "2024-10-03T00:22:42.505384Z",
     "iopub.status.idle": "2024-10-03T00:22:42.511072Z",
     "shell.execute_reply.started": "2024-10-03T00:22:42.505343Z",
     "shell.execute_reply": "2024-10-03T00:22:42.510084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "source": "predict_w_prompt(essay_response, prompt, model0)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T00:22:42.512493Z",
     "iopub.execute_input": "2024-10-03T00:22:42.513346Z",
     "iopub.status.idle": "2024-10-03T00:22:59.022923Z",
     "shell.execute_reply.started": "2024-10-03T00:22:42.513307Z",
     "shell.execute_reply": "2024-10-03T00:22:59.021919Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "index: 0\nvalue: 0.4849114418029785\nprobability: 0.4849114418029785\nindex: 0\nvalue: 0.6759265661239624\nprobability: 1.160838007926941\nindex: 0\nvalue: 0.8434841632843018\nprobability: 2.0043221712112427\nindex: 0\nvalue: 0.15197552740573883\nprobability: 2.1562976986169815\nindex: 19\nvalue: 0.004859927576035261\nprobability: 2.1611576261930168\nindex: 1\nvalue: 0.15160542726516724\nprobability: 2.312763053458184\nindex: 12\nvalue: 0.0079202214255929\nprobability: 2.320683274883777\nindex: 1\nvalue: 0.22824814915657043\nprobability: 2.5489314240403473\n",
     "output_type": "stream"
    },
    {
     "execution_count": 39,
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.3186164280050434"
     },
     "metadata": {}
    }
   ],
   "execution_count": 39
  }
 ]
}
